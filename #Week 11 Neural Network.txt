#Week 11: Neural Network
Code:

import numpy as np 
def sigmoid(x):
    return 1/(1+np.exp (-x))
def sigmoid_derivative(x):
    return x*(1-x)
X=np.array ([[0,0], [0,1], [1,0], [1,1]])
y=np.array ([[0], [1], [1], [0]])
input_layer_neurons=2
hidden_layer_neurons=4
output_layer_neurons=1
epochs=10000
learning_rate=0.1
np. random. seed (42)
wh=np.random.uniform(size=(input_layer_neurons,hidden_layer_neurons))
bh=np.random.uniform(size=(1,hidden_layer_neurons))
wout=np.random.uniform(size=(hidden_layer_neurons,output_layer_neurons))
bout=np.random.uniform(size=(1,output_layer_neurons))
for epoch in range (epochs) :
    hidden_layer_input=np.dot(X,wh)+bh
    hidden_layer_output=sigmoid (hidden_layer_input)
    output_layer_input=np.dot(hidden_layer_output,wout) +bout
    output=sigmoid (output_layer_input)
    error=y-output
    output_layer_gradient=sigmoid_derivative(output)
    d_output=error*output_layer_gradient
    hidden_layer_gradient=sigmoid_derivative(hidden_layer_output)
    d_hidden_layer=d_output.dot(wout.T) *hidden_layer_gradient
    wout+=hidden_layer_output.T.dot(d_output)*learning_rate
    bout+=np.sum(d_output,axis=0,keepdims=True)*learning_rate
    wh+=X.T.dot(d_hidden_layer)*learning_rate
    bh+=np.sum(d_hidden_layer, axis=0,keepdims=True)*learning_rate
    if epoch % 1000 == 0:
        print (f"Epoch{epoch}, Error: {np.mean(np.abs(error))}")
print("Final predictions after training:")
print (output)

output:-
Epoch0, Error: 0.49914791405546904
Epoch1000, Error: 0.4989908274224632
Epoch2000, Error: 0.49392112204426847
Epoch3000, Error: 0.46086324847622695
Epoch4000, Error: 0.37081148754970494
Epoch5000, Error: 0.2293685934150816
Epoch6000, Error: 0.1411700792664044
Epoch7000, Error: 0.10187019467760619
Epoch8000, Error: 0.08085064924133495
Epoch9000, Error: 0.06790718296112089
Final predictions after training:
[[0.04690963]
 [0.95663392]
 [0.92548675]
 [0.07177571]]
