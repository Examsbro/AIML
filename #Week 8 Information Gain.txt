#Week 8: Information Gain
import pandas as pd
import math
from collections import Counter
from pprint import pprint
def entropy(probs):
    return sum(-prob * math.log(prob, 2) for prob in probs if prob > 0)

def entropy_of_list(a_list):
    cnt = Counter(a_list)
    num_instances = len(a_list)
    probs = [x / num_instances for x in cnt.values()]
    return entropy(probs)

def information_gain(df, split_attribute_name, target_attribute_name):
    df_split = df.groupby(split_attribute_name)
    nobs = len(df.index) * 1.0
    df_agg_ent = df_split[target_attribute_name].agg([entropy_of_list, lambda x: len(x) / nobs])
    df_agg_ent.columns = ['Entropy', 'Proportion']
    avg_info = sum(df_agg_ent['Entropy'] * df_agg_ent['Proportion'])
    old_entropy = entropy_of_list(df[target_attribute_name])
    return old_entropy - avg_info

def id3DT(df, target_attribute_name, attribute_names, default_class=None):
    cnt = Counter(df[target_attribute_name])
    if len(cnt) == 1:  # All instances have the same class
        return next(iter(cnt))
    elif df.empty or not attribute_names:  # No data or attributes left
        return default_class
    else:
        default_class = max(cnt, key=cnt.get)  # Most common class
        gains = [information_gain(df, attr, target_attribute_name) for attr in attribute_names]
        index_of_max = gains.index(max(gains))
        best_attr = attribute_names[index_of_max]
        tree = {best_attr: {}}
        remaining_attributes = [attr for attr in attribute_names if attr != best_attr]
        for attr_val, data_subset in df.groupby(best_attr):
            subtree = id3DT(data_subset, target_attribute_name, remaining_attributes, default_class)
            tree[best_attr][attr_val] = subtree
        return tree

def classify(instance, tree, default=None):
    attribute = next(iter(tree))
    if instance[attribute] in tree[attribute]:
        result = tree[attribute][instance[attribute]]
        if isinstance(result, dict):
            return classify(instance, result)
        else:
            return result
    else:
        return default

data={
'Outlook':['Sunny','Sunny','Overcast','Rain','Rain','Rain','Overcast','Sunny','Sunny','Rain','Sunny','Overcast','Overcast','Rain'],
'Temperature':['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild'],
'Humidity':['High','High','High','High','Normal','Normal','Normal','High','Normal','Normal','Normal','High','Normal','High'],
'Wind':['Weak','Strong','Weak','Weak','Weak','Strong','Strong','Weak','Weak','Weak','Strong','Strong','Weak','Strong'],
'PlayTennis':['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']
}
df=pd.DataFrame(data)
df
attribute_names=list(df.columns)
attribute_names.remove('PlayTennis')
attribute_names
tree=id3DT(df,'PlayTennis',attribute_names)
print("The Resultant Decision Tree is:")
pprint(tree)
new_data={
'Outlook':['Rain','Sunny'],
'Temperature':['Mild','Hot'],
'Humidity':['High','Normal'],
'Wind':['Weak','Strong']
}
df2=pd.DataFrame(new_data)
df2
df2['Predicted']=df2.apply(classify,axis=1,args=(tree,'No'))
df2)

output:-
The Resultant Decision Tree is:
{'Outlook': {'Overcast': 'Yes',
             'Rain': {'Wind': {'Strong': 'No', 'Weak': 'Yes'}},
             'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}}}
Outlook	Temperature	Humidity	Wind	Predicted
0	Rain	Mild	High	Weak	Yes
1	Sunny	Hot	Normal	Strong	Yes
